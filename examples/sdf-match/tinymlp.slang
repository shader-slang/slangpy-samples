// SPDX-License-Identifier: Apache-2.0

import main;

public static const int input_size = 3;
public static const int hidden_size = 16;
public static const int output_size = 1;

public struct Layer<int input_size, int output_size>
{
    internal float* m_weights;
    internal float* m_biases;
    internal float* m_weights_grad;
    internal float* m_biases_grad;

    static const float LEAKY_RELU_ALPHA = 0.01f;

    internal __init(float* inWeights, float* inBiases, float* inWeightsGrad, float* inBiasesGrad)
    {
        m_weights = inWeights;
        m_biases = inBiases;
        m_weights_grad = inWeightsGrad;
        m_biases_grad = inBiasesGrad;
    }

    [BackwardDerivative(backward)]
    [NoDiffThis]
    public float[output_size] forward(float[input_size] input)
    {
        float[output_size] output;
        [ForceUnroll]
        for (int i = 0; i < output_size; i++) {
            float sum = 0.0f;
            [ForceUnroll]
            for (int j = 0; j < input_size; j++) {
                sum += m_weights[i * input_size + j] * input[j];
            }
            output[i] = sum + m_biases[i];

            // apply leaky relu on output
            if (output[i] < 0.0f)
            {
                output[i] *= LEAKY_RELU_ALPHA;
            }
        }
        return output;
    }

    // Backward pass of the layer.
    // dResult is the gradient from the last layer.
    public void backward(inout DifferentialPair<float[input_size]> input, float[output_size] dResult)
    {
        float pre_act = 0.0f;

        // gradient of leaky relu
        [ForceUnroll]
        for (int i = 0; i < output_size; i++)
        {
            float sum = 0.0f;
            [ForceUnroll]
            for (int j = 0; j < input_size; j++) {
                sum += m_weights[i * input_size + j] * input.p[j];
            }
            pre_act = sum + m_biases[i];

            if (pre_act < 0.0f) {
                dResult[i] *= LEAKY_RELU_ALPHA;
            }
        }

        // gradient of weights += input^T * dResult
        [ForceUnroll]
        for (int i = 0; i < input_size; i++)
        {
            [ForceUnroll]
            for (int j = 0; j < output_size; j++)
            {
                // Gradient of weights is shared by all input elements, so accumulation has to be atomic
                __atomic_add(m_weights_grad[j * input_size + i], dResult[j] * input.p[i]);
            }
        }

        // gradient of biases += dResult
        [ForceUnroll]
        for (int i = 0; i < output_size; i++)
        {
            // Gradient of biases is shared by all input elements, so accumulation has to be atomic
            __atomic_add(m_biases_grad[i], dResult[i]);
        }

        // gradient of input = weight^T * dResult
        float[input_size] dInput = {};

        [ForceUnroll]
        for (int i = 0; i < input_size; i++)
        {
            float sum = 0.0f;
            [ForceUnroll]
            for (int j = 0; j < output_size; j++)
            {
                sum += m_weights[j * input_size + i] * dResult[j];
            }
            dInput[i] += sum;
        }

        input = {input.p, dInput};
    }
}

public struct TinyMLP
{
    internal Layer<input_size, hidden_size> m_layer1;
    internal Layer<hidden_size, output_size> m_layer2;

    // Construct the TinyMLP from the shader parameters, TinyMLP_Params is a struct that contains the parameters and gradients.
    // We will use the shape of layer to calculate the offset of the parameters and gradients.
    public __init(TinyMLP_Params* params)
    {
        {
            constexpr int weights_stride = input_size * hidden_size;
            constexpr int biases_offset = weights_stride;
            m_layer1 = Layer<input_size, hidden_size>(params->m_params, params->m_params + biases_offset,
                params->m_grads, params->m_grads + biases_offset);
        }

        {
            constexpr int layer_offset = (input_size * hidden_size) + hidden_size;  // offset of layer 2
            constexpr int weights_stride = hidden_size * output_size;               // stride of weights of layer 2
            constexpr int biases_offset = layer_offset + weights_stride;            // offset of biases of layer 2
            m_layer2 = Layer<hidden_size, output_size>(params->m_params + layer_offset, params->m_params + biases_offset,
                params->m_grads + layer_offset, params->m_grads + biases_offset);
        }
    }

    // Forward pass of the TinyMLP.
    // input is the input of the MLP.
    // output is the output of the MLP.
    // Backward pass is automatically generated by Slang.
    [Differentiable]
    [NoDiffThis]
    public float[output_size] eval(float[input_size] input)
    {
        float[hidden_size] hidden = m_layer1.forward(input);
        return m_layer2.forward(hidden);
    }
}
